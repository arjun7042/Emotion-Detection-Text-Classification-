{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52664980",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\arjun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\arjun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\arjun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing Libraries\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c634e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ab925e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nonaug = pd.read_csv('emotion_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ceb715de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nonaug.drop(['Unnamed: 0', 'Unnamed: 3'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cba85f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nonaug.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ea46f06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31161, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nonaug.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "754d748c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning on non augmented dataset\n",
    "# some text cleaning functions\n",
    "def convert_to_lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_numbers(text):\n",
    "    number_pattern = r'\\d+'\n",
    "    without_number = re.sub(pattern=number_pattern, repl=\" \", string=text)\n",
    "    return without_number\n",
    "\n",
    "def lemmatizing(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)\n",
    "    for i in range(len(tokens)):\n",
    "        lemma_word = lemmatizer.lemmatize(tokens[i])\n",
    "        tokens[i] = lemma_word\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    removed = []\n",
    "    stop_words = list(stopwords.words(\"english\"))\n",
    "    tokens = word_tokenize(text)\n",
    "    for i in range(len(tokens)):\n",
    "        if tokens[i] not in stop_words:\n",
    "            removed.append(tokens[i])\n",
    "    return \" \".join(removed)\n",
    "\n",
    "def remove_extra_white_spaces(text):\n",
    "    single_char_pattern = r'\\s+[a-zA-Z]\\s+'\n",
    "    without_sc = re.sub(pattern=single_char_pattern, repl=\" \", string=text)\n",
    "    return without_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9dade491",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nonaug['Text'] = df_nonaug['Text'].apply(lambda x: convert_to_lower(x))\n",
    "df_nonaug['Text'] = df_nonaug['Text'].apply(lambda x: remove_numbers(x))\n",
    "df_nonaug['Text'] = df_nonaug['Text'].apply(lambda x: remove_punctuation(x))\n",
    "df_nonaug['Text'] = df_nonaug['Text'].apply(lambda x: remove_stopwords(x))\n",
    "df_nonaug['Text'] = df_nonaug['Text'].apply(lambda x: remove_extra_white_spaces(x))\n",
    "df_nonaug['Text'] = df_nonaug['Text'].apply(lambda x: lemmatizing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4271cb97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>joy</td>\n",
       "      <td>sage act upgrade list tommorow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sadness</td>\n",
       "      <td>way homegirl baby funeral man hate funeral rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>joy</td>\n",
       "      <td>eye true hazel eyeand brilliant regular featur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>joy</td>\n",
       "      <td>iluvmiasantos ugh babe hugggzzz babe naamazed ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Emotion                                               Text\n",
       "0  neutral                                                   \n",
       "1      joy                     sage act upgrade list tommorow\n",
       "2  sadness  way homegirl baby funeral man hate funeral rea...\n",
       "3      joy  eye true hazel eyeand brilliant regular featur...\n",
       "4      joy  iluvmiasantos ugh babe hugggzzz babe naamazed ..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nonaug.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8a04bee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>sage act upgrade list tommorow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>way homegirl baby funeral man hate funeral rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>eye true hazel eyeand brilliant regular featur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>iluvmiasantos ugh babe hugggzzz babe naamazed ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Emotion                                               Text\n",
       "0        5                                                   \n",
       "1        0                     sage act upgrade list tommorow\n",
       "2        1  way homegirl baby funeral man hate funeral rea...\n",
       "3        0  eye true hazel eyeand brilliant regular featur...\n",
       "4        0  iluvmiasantos ugh babe hugggzzz babe naamazed ..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting string labels to int labels\n",
    "\n",
    "label_map = {\n",
    "    'joy': 0,\n",
    "    'sadness': 1,\n",
    "    'fear' : 2,\n",
    "    'surprise' : 3,\n",
    "    'anger' : 4,\n",
    "    'neutral' : 5,\n",
    "    'disgust' : 6,\n",
    "    'shame' : 7\n",
    "}\n",
    "\n",
    "df_nonaug['Emotion'] = df_nonaug['Emotion'].map(label_map)\n",
    "df_nonaug.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ced85e7",
   "metadata": {},
   "source": [
    "#### Importing augmented dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f20ade2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aug = pd.read_csv('aug_df6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9cefe08c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60520</td>\n",
       "      <td>1</td>\n",
       "      <td>every sport team cheer kick straight sack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33981</td>\n",
       "      <td>4</td>\n",
       "      <td>mad mad mad mad mad mad mad mad mad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>73997</td>\n",
       "      <td>7</td>\n",
       "      <td>civil service servant watched barely concealed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56999</td>\n",
       "      <td>3</td>\n",
       "      <td>to get home kids twitter app completely different</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>62764</td>\n",
       "      <td>2</td>\n",
       "      <td>left alone home man down forced upstairs front...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Emotion                                               Text\n",
       "0       60520        1          every sport team cheer kick straight sack\n",
       "1       33981        4                mad mad mad mad mad mad mad mad mad\n",
       "2       73997        7  civil service servant watched barely concealed...\n",
       "3       56999        3  to get home kids twitter app completely different\n",
       "4       62764        2  left alone home man down forced upstairs front..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_aug.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7622445",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aug.drop(['Unnamed: 0'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34b6f1de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Emotion      0\n",
       "Text       914\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_aug.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "acf84326",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aug.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20e65984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>every sport team cheer kick straight sack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>mad mad mad mad mad mad mad mad mad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>civil service servant watched barely concealed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>to get home kids twitter app completely different</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>left alone home man down forced upstairs front...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Emotion                                               Text\n",
       "0        1          every sport team cheer kick straight sack\n",
       "1        4                mad mad mad mad mad mad mad mad mad\n",
       "2        7  civil service servant watched barely concealed...\n",
       "3        3  to get home kids twitter app completely different\n",
       "4        2  left alone home man down forced upstairs front..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_aug.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f578462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80548, 2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_aug.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15017de3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    10458\n",
       "1    10373\n",
       "6    10355\n",
       "2    10266\n",
       "3    10215\n",
       "4    10160\n",
       "7     9644\n",
       "5     9077\n",
       "Name: Emotion, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_aug['Emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5930c9",
   "metadata": {},
   "source": [
    "In our augmented dataset, roughly same number of data points belong to each class. So the problem of class imbalance is somewhat solved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360dc9f6",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32348fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset before augmenting to avoid augmented data in valid set\n",
    "X_train, _, y_train, _ = train_test_split(df_aug['Text'], df_aug['Emotion'].values, test_size=0.1, stratify=df_aug['Emotion'].values)\n",
    "_, X_test, _, y_test = train_test_split(df_nonaug['Text'], df_nonaug['Emotion'].values, test_size=0.5,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01c8a5c",
   "metadata": {},
   "source": [
    "We dont use augmented dataset for testing purpose and we always test our model on original dataset. Therefore train test split is happened like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90a078f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72493,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ff276e44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15581,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2533d14",
   "metadata": {},
   "source": [
    "### Text Vectorization using tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df350dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f1662b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "X_train_tf = tfidf.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e337b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tf = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "daf30a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(tfidf, open('vectorizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51572d8d",
   "metadata": {},
   "source": [
    "# ML Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bd2fb4",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2c2d77a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8b0c74e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ce6c04a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_classifier = KNeighborsClassifier(n_neighbors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "914cf011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_classifier.fit(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b980aca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds_knn = knn_classifier.predict(X_test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "606b1c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 710  196   25   18   16 4212    5    1]\n",
      " [  30 1249   13    5   29 1775    2    3]\n",
      " [  10   34 1314    0    7  820    1    0]\n",
      " [  10   41    0 1106    0  867    0    0]\n",
      " [   6   35    5    2 1175  636    1    0]\n",
      " [   3    5    2    0    1  709    0    1]\n",
      " [   0    0    0    0    0    0  423    0]\n",
      " [   0    0    0    0    0    0    0   78]]\n",
      "0.4341184776330146\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.14      0.24      5183\n",
      "           1       0.80      0.40      0.54      3106\n",
      "           2       0.97      0.60      0.74      2186\n",
      "           3       0.98      0.55      0.70      2024\n",
      "           4       0.96      0.63      0.76      1860\n",
      "           5       0.08      0.98      0.15       721\n",
      "           6       0.98      1.00      0.99       423\n",
      "           7       0.94      1.00      0.97        78\n",
      "\n",
      "    accuracy                           0.43     15581\n",
      "   macro avg       0.83      0.66      0.64     15581\n",
      "weighted avg       0.88      0.43      0.51     15581\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_preds_knn))\n",
    "print(accuracy_score(y_test, y_preds_knn))\n",
    "print(classification_report(y_test, y_preds_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e02117c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b03a1aa1",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4f4d2263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nv_model = MultinomialNB()\n",
    "nv_model.fit(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b63ba16",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tf = tfidf.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8abc50a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((72493, 38066), (15581, 38066))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tf.shape, X_test_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "fa7f15bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = nv_model.predict(X_test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "65732a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4110  269  193  144  182  145  161   38]\n",
      " [ 279 2249   94   61  178   65  115   24]\n",
      " [ 104  115 1684   34   84   20   63   20]\n",
      " [ 228   93   29 1456   65   33   76   13]\n",
      " [  89  112   47   20 1489   33   85   21]\n",
      " [ 123   34   29   13   22  461   40   25]\n",
      " [  11    3    1    3    3    0  419    0]\n",
      " [   0    0    0    0    0    0    0   74]]\n",
      "0.7664463128168924\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.78      0.81      5242\n",
      "           1       0.78      0.73      0.76      3065\n",
      "           2       0.81      0.79      0.80      2124\n",
      "           3       0.84      0.73      0.78      1993\n",
      "           4       0.74      0.79      0.76      1896\n",
      "           5       0.61      0.62      0.61       747\n",
      "           6       0.44      0.95      0.60       440\n",
      "           7       0.34      1.00      0.51        74\n",
      "\n",
      "    accuracy                           0.77     15581\n",
      "   macro avg       0.67      0.80      0.70     15581\n",
      "weighted avg       0.78      0.77      0.77     15581\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_preds))\n",
    "print(accuracy_score(y_test, y_preds))\n",
    "print(classification_report(y_test, y_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "32caeee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text =['he hates running all day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "3da4f516",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = tfidf.transform(sample_text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "65c7d82e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5], dtype=int64)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nv_model.predict(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3596c17e",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8b69b930",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f3153f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "45871977",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arjun\\anaconda3\\envs\\EmotionDetection\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "9a2b6626",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds1 = lr.predict(X_test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9772ebf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4363  230  126  168  111  200   43    1]\n",
      " [ 395 2229   88   93  137   89   30    4]\n",
      " [ 175  120 1682   43   60   30   14    0]\n",
      " [ 303   97   24 1480   35   41   13    0]\n",
      " [ 177  127   51   38 1420   55   27    1]\n",
      " [ 125   32   14   10   13  548    5    0]\n",
      " [  11    7    2    2    3    0  415    0]\n",
      " [   0    0    0    0    0    0    0   74]]\n",
      "0.7837109299788203\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.83      0.81      5242\n",
      "           1       0.78      0.73      0.75      3065\n",
      "           2       0.85      0.79      0.82      2124\n",
      "           3       0.81      0.74      0.77      1993\n",
      "           4       0.80      0.75      0.77      1896\n",
      "           5       0.57      0.73      0.64       747\n",
      "           6       0.76      0.94      0.84       440\n",
      "           7       0.93      1.00      0.96        74\n",
      "\n",
      "    accuracy                           0.78     15581\n",
      "   macro avg       0.78      0.81      0.80     15581\n",
      "weighted avg       0.79      0.78      0.78     15581\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_preds1))\n",
    "print(accuracy_score(y_test, y_preds1))\n",
    "print(classification_report(y_test, y_preds1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b672238",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b1000eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2bfe9266",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2ac4f77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arjun\\anaconda3\\envs\\EmotionDetection\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:38:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "              gamma=0, gpu_id=-1, importance_type=None,\n",
       "              interaction_constraints='', learning_rate=0.300000012,\n",
       "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=100, n_jobs=16,\n",
       "              num_parallel_tree=1, objective='multi:softprob', predictor='auto',\n",
       "              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=None,\n",
       "              subsample=1, tree_method='exact', validate_parameters=1,\n",
       "              verbosity=None)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.fit(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "16692020",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds2 = xgb.predict(X_test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f363b4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4379  216   90  227   99  198   87    0]\n",
      " [1020 1612   65  120  100   96   87    1]\n",
      " [ 632   78 1221   66   39   50   53    0]\n",
      " [ 712   59   25 1061   36   27   35    0]\n",
      " [ 540  107   23   19 1069   66   63    0]\n",
      " [ 278   16    4    2    4  407    3    0]\n",
      " [  82   12    1    3    3    4  324    0]\n",
      " [   0    0    0    0    0    0    0   60]]\n",
      "0.6503433669212503\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.83      0.68      5296\n",
      "           1       0.77      0.52      0.62      3101\n",
      "           2       0.85      0.57      0.68      2139\n",
      "           3       0.71      0.54      0.61      1955\n",
      "           4       0.79      0.57      0.66      1887\n",
      "           5       0.48      0.57      0.52       714\n",
      "           6       0.50      0.76      0.60       429\n",
      "           7       0.98      1.00      0.99        60\n",
      "\n",
      "    accuracy                           0.65     15581\n",
      "   macro avg       0.71      0.67      0.67     15581\n",
      "weighted avg       0.69      0.65      0.65     15581\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_preds2))\n",
    "print(accuracy_score(y_test, y_preds2))\n",
    "print(classification_report(y_test, y_preds2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2a7ceb",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "a91d3a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "25987192",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_classifier = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "e49b9b41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_classifier.fit(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "0bb51843",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds3 = dt_classifier.predict(X_test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "6731247b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5021   63   18   46   29   55   10    0]\n",
      " [  77 2886   19   17   35   27    4    0]\n",
      " [  40   19 2036    6   10    9    4    0]\n",
      " [  53   12    6 1905    8    4    5    0]\n",
      " [  35   27   12   11 1790   17    4    0]\n",
      " [   9    8    2    1    7  719    1    0]\n",
      " [   0    0    1    1    1    1  436    0]\n",
      " [   0    0    0    0    0    0    0   74]]\n",
      "0.9541749566780052\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96      5242\n",
      "           1       0.96      0.94      0.95      3065\n",
      "           2       0.97      0.96      0.97      2124\n",
      "           3       0.96      0.96      0.96      1993\n",
      "           4       0.95      0.94      0.95      1896\n",
      "           5       0.86      0.96      0.91       747\n",
      "           6       0.94      0.99      0.96       440\n",
      "           7       1.00      1.00      1.00        74\n",
      "\n",
      "    accuracy                           0.95     15581\n",
      "   macro avg       0.95      0.96      0.96     15581\n",
      "weighted avg       0.95      0.95      0.95     15581\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_preds3))\n",
    "print(accuracy_score(y_test, y_preds3))\n",
    "print(classification_report(y_test, y_preds3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "df94a496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5], dtype=int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_classifier.predict(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d8fb4775",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text =['i will fight you']\n",
    "vect = tfidf.transform(sample_text).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24eb780c",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b24ebab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd0c08d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "675d6341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_classifier.fit(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cc52be17",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds4 = rf_classifier.predict(X_test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a6dea460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5146   34    7   10   18   77    4    0]\n",
      " [  75 2969    8    7   19   22    1    0]\n",
      " [  26   12 2074    6    5   16    0    0]\n",
      " [  44    5    4 1882    4   15    1    0]\n",
      " [  27   15    2    3 1816   24    0    0]\n",
      " [   6    2    2    0    3  701    0    0]\n",
      " [   1    0    0    0    0    0  428    0]\n",
      " [   0    0    0    0    0    0    0   60]]\n",
      "0.967588729863295\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97      5296\n",
      "           1       0.98      0.96      0.97      3101\n",
      "           2       0.99      0.97      0.98      2139\n",
      "           3       0.99      0.96      0.97      1955\n",
      "           4       0.97      0.96      0.97      1887\n",
      "           5       0.82      0.98      0.89       714\n",
      "           6       0.99      1.00      0.99       429\n",
      "           7       1.00      1.00      1.00        60\n",
      "\n",
      "    accuracy                           0.97     15581\n",
      "   macro avg       0.96      0.98      0.97     15581\n",
      "weighted avg       0.97      0.97      0.97     15581\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_preds4))\n",
    "print(accuracy_score(y_test, y_preds4))\n",
    "print(classification_report(y_test, y_preds4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd967113",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization for Random Forest using RandomSearch CV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37626e40",
   "metadata": {},
   "source": [
    "The main parameters used by a Random Forest Classifier are:\n",
    "\n",
    "+ criterion = the function used to evaluate the quality of a split.\n",
    "+ max_depth = maximum number of levels allowed in each tree.\n",
    "+ max_features = maximum number of features considered when splitting a node.\n",
    "+ min_samples_leaf = minimum number of samples which can be stored in a tree leaf.\n",
    "+ min_samples_split = minimum number of samples necessary in a node to cause node splitting.\n",
    "+ n_estimators = number of trees in the ensamble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "099e72f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'max_features': ['auto', 'sqrt', 'log2'], 'max_depth': [10, 120, 230, 340, 450, 560, 670, 780, 890, 1000], 'min_samples_split': [2, 5, 10, 14], 'min_samples_leaf': [1, 2, 4, 6, 8], 'criterion': ['entropy', 'gini']}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt','log2']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 1000,10)]\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10,14]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4,6,8]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "              'criterion':['entropy','gini']}\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f3092c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arjun\\anaconda3\\envs\\EmotionDetection\\lib\\site-packages\\sklearn\\model_selection\\_search.py:925: UserWarning: One or more of the test scores are non-finite: [0.65035241 0.55590189 0.77505413 0.48930236 0.47194901 0.55391548\n",
      " 0.4718387  0.60717586 0.6521043  0.76426687 0.59480224 0.57831785\n",
      " 0.66333297 0.62072198 0.54107287 0.57606937 0.46861074 0.60447213\n",
      " 0.60492737 0.57938004 0.48012909 0.77346776 0.5782351  0.59542298\n",
      " 0.69206678 0.5789386  0.7929455  0.56679951 0.68765254 0.78682079\n",
      " 0.67025778 0.56078517 0.77755091 0.62109443 0.37305658 0.60481701\n",
      " 0.57840063 0.87771232 0.65146975        nan 0.49526159 0.57184826\n",
      " 0.56038514 0.58724287 0.8008635  0.62138411 0.64700034        nan\n",
      " 0.57837303 0.36151081 0.59524366 0.58779464 0.76596359 0.80050484\n",
      " 0.65425625 0.67271327 0.56406832 0.65206293 0.58724287 0.46410006\n",
      " 0.57888344 0.60440317 0.8695736  0.87884346 0.76607396 0.48463987\n",
      " 0.42773787        nan 0.60405832 0.56358548 0.56141972 0.87713297\n",
      " 0.78501369 0.76992257 0.67476856 0.87128411 0.59531265 0.59521607\n",
      " 0.57706255 0.86718716 0.44953292 0.67103026 0.66759545 0.37164966\n",
      " 0.73553307 0.47185241 0.6035065  0.51982961 0.62157724 0.48425362\n",
      " 0.60746554 0.60314784 0.59287102 0.76706713 0.80914016 0.62142549\n",
      " 0.79486295 0.61948049 0.86892525 0.60243054]\n",
      "  category=UserWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=RandomForestClassifier(), n_iter=100,\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'criterion': ['entropy', 'gini'],\n",
       "                                        'max_depth': [10, 120, 230, 340, 450,\n",
       "                                                      560, 670, 780, 890,\n",
       "                                                      1000],\n",
       "                                        'max_features': ['auto', 'sqrt',\n",
       "                                                         'log2'],\n",
       "                                        'min_samples_leaf': [1, 2, 4, 6, 8],\n",
       "                                        'min_samples_split': [2, 5, 10, 14],\n",
       "                                        'n_estimators': [200, 400, 600, 800,\n",
       "                                                         1000, 1200, 1400, 1600,\n",
       "                                                         1800, 2000]},\n",
       "                   random_state=100, verbose=2)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf=RandomForestClassifier()\n",
    "rf_randomcv=RandomizedSearchCV(estimator=rf,param_distributions=random_grid,n_iter=100,cv=3,verbose=2,\n",
    "                               random_state=100,n_jobs=-1)\n",
    "### fit the randomized model\n",
    "rf_randomcv.fit(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "18ea5edd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 2000,\n",
       " 'min_samples_split': 2,\n",
       " 'min_samples_leaf': 1,\n",
       " 'max_features': 'log2',\n",
       " 'max_depth': 340,\n",
       " 'criterion': 'gini'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_randomcv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "20154426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=340, max_features='log2', n_estimators=2000)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_randomcv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9137c8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_random_grid = rf_randomcv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "78377488",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier_tuned = RandomForestClassifier(n_estimators=2000, min_samples_split=2, min_samples_leaf=1, max_features='log2', \n",
    "                                             max_depth = 340, criterion= 'gini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1cc8e905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=340, max_features='log2', n_estimators=2000)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_classifier_tuned.fit(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "af8c6f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds_tuned = rf_classifier_tuned.predict(X_test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700bac6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7d6d3846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5196   31    8    6    7   46    2    0]\n",
      " [  76 2987    6    8    7   15    2    0]\n",
      " [  36    2 2090    3    2    6    0    0]\n",
      " [  52    3    2 1890    2    6    0    0]\n",
      " [  23    8    2    2 1839   13    0    0]\n",
      " [  88    3    2    0    1  620    0    0]\n",
      " [   0    0    0    0    0    0  429    0]\n",
      " [   0    0    0    0    0    0    0   60]]\n",
      "0.969835055516334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.98      0.97      5296\n",
      "           1       0.98      0.96      0.97      3101\n",
      "           2       0.99      0.98      0.98      2139\n",
      "           3       0.99      0.97      0.98      1955\n",
      "           4       0.99      0.97      0.98      1887\n",
      "           5       0.88      0.87      0.87       714\n",
      "           6       0.99      1.00      1.00       429\n",
      "           7       1.00      1.00      1.00        60\n",
      "\n",
      "    accuracy                           0.97     15581\n",
      "   macro avg       0.97      0.97      0.97     15581\n",
      "weighted avg       0.97      0.97      0.97     15581\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_preds_tuned))\n",
    "print(accuracy_score(y_test, y_preds_tuned))\n",
    "print(classification_report(y_test, y_preds_tuned))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5da7424",
   "metadata": {},
   "source": [
    "- Not much improvement in accuracy (increased from 96.75% to 96.98%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "66a94070",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text =['i want to fight you']\n",
    "vect = tfidf.transform(sample_text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "93684cd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4], dtype=int64)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_classifier_tuned.predict(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07bce9f",
   "metadata": {},
   "source": [
    "#### Our best performing model is Random Forest Classifier after performing hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c251697c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "83b94187",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"emotion_detection_model.pkl\", \"wb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "698838c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(rf_classifier_tuned, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "93fa9d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630f1920",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
